{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0774cbb0",
   "metadata": {},
   "source": [
    "#### Author: Aditya Jain\n",
    "#### Topic : TD(n) and Mountain Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff27d7e3-421d-48e3-a606-aae1d7a97d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display as ipythondisplay\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611175a-e237-4cdd-ac7c-7fe2e852777d",
   "metadata": {},
   "source": [
    "#### Exercise 1: Prelims\n",
    "Here we explore the mountain car environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bc4da3-98a6-4038-8a48-46cf750fa2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action Space:  Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "print('Observation Space: ', env.observation_space)\n",
    "print('Action Space: ', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5a4e6-57a2-4221-8d30-c049fd62df51",
   "metadata": {},
   "source": [
    "The environment's state space is represented by two variables: car position and car velocity. The car position can vary from -1.2 to 0.6 and velocity varies from -0.07 to 0.07.\n",
    "The agent can choose to take 3 actions: accelerate left (0), accelerate right (2) or don't accelerate (1). The environment's model is not available, hence, we cannot use DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1342b241-574e-4f1e-9cc4-3eb4735626cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for _ in range(10):\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "    screen = env.render(mode='rgb_array')  \n",
    "    plt.imshow(screen)\n",
    "    \n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c48497-247c-497a-a3dc-357a5c69ebe4",
   "metadata": {},
   "source": [
    "#### Check Controller Performance\n",
    "The below function is used to check the RL controller over n (=1000) trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad91567-ff66-4d40-a392-a709aa1998a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_performance(env_name, pos_fac, vel_fac, policy, no_actions, trials):\n",
    "    \n",
    "    env          = gym.make(env_name)\n",
    "    reward_list  = []\n",
    "    steps_list   = []\n",
    "    reached_goal = 0\n",
    "    \n",
    "    for i in range(trials):\n",
    "        done           = False\n",
    "        episode_reward = 0\n",
    "        obs            = env.reset()\n",
    "        obs            = process_state(obs, pos_fac, vel_fac)\n",
    "        steps          = 0\n",
    "        \n",
    "        while not done:\n",
    "            action                  = np.random.choice(no_actions, p=policy[obs])\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            steps                   += 1\n",
    "            episode_reward          += reward\n",
    "            obs                     = process_state(obs, pos_fac, vel_fac)\n",
    "            \n",
    "        if steps<200:\n",
    "            steps_list.append(steps)\n",
    "            reached_goal += 1            \n",
    "            \n",
    "        reward_list.append(episode_reward)\n",
    "        \n",
    "    return reached_goal, steps_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df91f5-27f6-4526-bd0f-08be0548bb23",
   "metadata": {},
   "source": [
    "Defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44a2e24-785e-44dd-a930-1573c0c67a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_policy(obs, epsilon, no_actions, policy): \n",
    "    '''chooses epsilon greedy action given a policy'''\n",
    "    \n",
    "    if np.random.rand()<epsilon:\n",
    "        action = np.random.choice(no_actions, p=[1/no_actions, 1/no_actions, 1/no_actions]) \n",
    "    else:\n",
    "        action = np.random.choice(no_actions, p=policy[obs]) \n",
    "    \n",
    "    return action\n",
    "    \n",
    "    \n",
    "def process_state(state, p_fac, v_fac):\n",
    "    return (round(state[0]*p_fac), round(state[1]*v_fac))\n",
    "\n",
    "\n",
    "def epsilon_greedy_action_Q(obs, epsilon, no_actions, Qpi_sa): \n",
    "    '''chooses epsilon greedy action given Q function'''\n",
    "    \n",
    "    if np.random.rand()<epsilon:  # random action \n",
    "        action = np.random.choice(no_actions, p=[1/no_actions, 1/no_actions, 1/no_actions]) \n",
    "    else:                         # greedy actions\n",
    "        qpi_list = []        \n",
    "        for action in range(no_actions):\n",
    "            qpi_list.append(Qpi_sa[obs, action])\n",
    "            \n",
    "        maxa_list = np.argwhere(qpi_list == np.amax(qpi_list))\n",
    "        maxa_list_indx = []\n",
    "        \n",
    "        # indices that have max. q values\n",
    "        for item in maxa_list:\n",
    "            maxa_list_indx.append(item[0])\n",
    "            \n",
    "        act_probab = [0, 0, 0]\n",
    "        for i in range(no_actions):\n",
    "            if i in maxa_list_indx:\n",
    "                act_probab[i] = 1/len(maxa_list_indx)\n",
    "            else:\n",
    "                act_probab[i] = 0\n",
    "        \n",
    "        action = np.random.choice(no_actions, p=act_probab) \n",
    "    \n",
    "    return action\n",
    "\n",
    "def get_policy(state_list, Qpi_sa):\n",
    "    ''''\n",
    "    based on the Q function, returns the policy\n",
    "    '''\n",
    "    \n",
    "    policy = {}    \n",
    "    \n",
    "    for state in state_list:\n",
    "        qpi_list = []\n",
    "        \n",
    "        for action in range(no_actions):\n",
    "            qpi_list.append(Qpi_sa[state, action])\n",
    "        \n",
    "        maxa_list      = np.argwhere(qpi_list == np.amax(qpi_list))\n",
    "        maxa_list_indx = []\n",
    "        \n",
    "        # indices that have max. q values\n",
    "        for item in maxa_list:\n",
    "            maxa_list_indx.append(item[0])\n",
    "            \n",
    "        # updating the new policy policy\n",
    "        policy[state] = [0, 0, 0]\n",
    "        for i in range(no_actions):\n",
    "            if i in maxa_list_indx:\n",
    "                policy[state][i] = 1/len(maxa_list_indx)\n",
    "            else:\n",
    "                policy[state][i] = 0\n",
    "                \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fafbb-4baf-4f23-8e87-a33a3ce5e8ef",
   "metadata": {},
   "source": [
    "##### Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63720dd4-5398-4ec9-9121-5fcd7490b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate of random policy over 1000 runs:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "random_policy_pi = {}\n",
    "\n",
    "for item in state_list:\n",
    "    random_policy_pi[item] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2\n",
    "    \n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, random_policy_pi, no_actions, 1000)\n",
    "print('Success rate of random policy over 1000 runs: ', (success/1000)*100, '%')\n",
    "if len(steps)!=0:    \n",
    "    print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c042d62-670e-452c-87b8-32ddbd8c402b",
   "metadata": {},
   "source": [
    "#### Task 1: TD(0) On-Policy SARSA\n",
    "Intialization of an arbitrary state-action value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e568c2c-b725-4707-8745-3484a3fa8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi_sa     = {}\n",
    "state_list = [] # contains the list of states\n",
    "no_actions = 3   # 0, 1 and 2\n",
    "\n",
    "# multiplicative factor of 100\n",
    "pos_min    = -120\n",
    "pos_max    = 60\n",
    "pos_inc    = 1\n",
    "pos_fac    = 100\n",
    "\n",
    "# multiplicative factor of 100\n",
    "vel_min    = -7\n",
    "vel_max    = 7\n",
    "vel_inc    = 1\n",
    "vel_fac    = 100\n",
    "\n",
    "for pos in range(pos_min, pos_max+1, pos_inc):\n",
    "    for vel in range(vel_min, vel_max+1, vel_inc):\n",
    "        state_list.append((pos, vel))\n",
    "        for action in range(no_actions):\n",
    "            Qpi_sa[(pos, vel), action] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c71bf6-db9a-4e6b-b5d9-870ea290ebfc",
   "metadata": {},
   "source": [
    "Intialization of an arbitrary policy based on Qpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2125efa-9f5e-47c1-a013-1414c50031fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi = {}\n",
    "\n",
    "for key in Qpi_sa.keys():\n",
    "    policy_pi[key[0]] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66887c97-6de4-4509-986b-314388960905",
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make('MountainCar-v0')\n",
    "\n",
    "episodes    = 0\n",
    "epsilon     = 0.1\n",
    "alpha       = 0.5\n",
    "gamma       = 0.9\n",
    "converged   = False\n",
    "\n",
    "while not converged:\n",
    "    episodes += 1\n",
    "    \n",
    "    cur_obs     = env.reset()\n",
    "    cur_obs     = process_state(cur_obs, pos_fac, vel_fac)\n",
    "    cur_action  = epsilon_greedy_action_policy(cur_obs, epsilon, no_actions, policy_pi)\n",
    "    done        = False\n",
    "    \n",
    "    while not done:        \n",
    "        next_obs, reward, done, info = env.step(cur_action)\n",
    "        next_obs                     = process_state(next_obs, pos_fac, vel_fac)\n",
    "        next_action                  = epsilon_greedy_action_policy(next_obs, epsilon, no_actions, policy_pi)\n",
    "        Qpi_sa[cur_obs,cur_action]   = Qpi_sa[cur_obs,cur_action] + alpha*(reward + gamma*Qpi_sa[next_obs,next_action] - Qpi_sa[cur_obs,cur_action])\n",
    "        cur_obs                      = next_obs\n",
    "        cur_action                   = next_action\n",
    "        \n",
    "    new_policy  = get_policy(state_list, Qpi_sa)    \n",
    "    if new_policy==policy_pi:\n",
    "        converged = True\n",
    "        \n",
    "    policy_pi = new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "421bd28c-ad2f-495a-83e4-9f6a36714b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes took to converge for on-policy SARSA:  11413\n",
      "Success rate of on-policy SARSA control over 1000 runs:  6.5 %\n",
      "Average steps taken to reach the goal:  178.56923076923076\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(policy_pi, open(\"task1_onpolicy-sarsa.p\", \"wb\" ))\n",
    "print('Episodes took to converge for on-policy SARSA: ', episodes)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy_pi, no_actions, 1000)\n",
    "print('Success rate of on-policy SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4babd04-56de-4322-8289-dfe64c81d359",
   "metadata": {},
   "source": [
    "#### Task 1: TD(0) On-Policy Expected SARSA\n",
    "Intialization of an arbitrary state-action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f178ca62-b11f-4d4a-bd34-461c0ef1fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi_sa     = {}\n",
    "state_list = [] # contains the list of states\n",
    "no_actions = 3   # 0, 1 and 2\n",
    "\n",
    "# multiplicative factor of 100\n",
    "pos_min    = -120\n",
    "pos_max    = 60\n",
    "pos_inc    = 1\n",
    "pos_fac    = 100\n",
    "\n",
    "# multiplicative factor of 100\n",
    "vel_min    = -7\n",
    "vel_max    = 7\n",
    "vel_inc    = 1\n",
    "vel_fac    = 100\n",
    "\n",
    "for pos in range(pos_min, pos_max+1, pos_inc):\n",
    "    for vel in range(vel_min, vel_max+1, vel_inc):\n",
    "        state_list.append((pos, vel))\n",
    "        for action in range(no_actions):\n",
    "            Qpi_sa[(pos, vel), action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5fd83633-9d02-4841-894b-a754a4cdf86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi = {}\n",
    "\n",
    "for item in state_list:\n",
    "    policy_pi[item] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63c020c0-a5a8-4e3f-af02-e80bb4417535",
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make('MountainCar-v0')\n",
    "\n",
    "episodes    = 0\n",
    "epsilon     = 0.1\n",
    "alpha       = 0.5\n",
    "gamma       = 0.9\n",
    "converged   = False\n",
    "\n",
    "while not converged:\n",
    "    episodes += 1\n",
    "    \n",
    "    # Policy Evaluation\n",
    "    cur_obs     = env.reset()\n",
    "    cur_obs     = process_state(cur_obs, pos_fac, vel_fac)\n",
    "    cur_action  = epsilon_greedy_action_policy(cur_obs, epsilon, no_actions, policy_pi)\n",
    "    done        = False\n",
    "    \n",
    "    while not done:        \n",
    "        next_obs, reward, done, info = env.step(cur_action)\n",
    "        next_obs                     = process_state(next_obs, pos_fac, vel_fac)\n",
    "        next_action                  = epsilon_greedy_action_policy(next_obs, epsilon, no_actions, policy_pi)\n",
    "        expect_q_sum                 = 0\n",
    "        for action in range(no_actions):\n",
    "            expect_q_sum             += policy_pi[next_obs][action]* Qpi_sa[next_obs, action]\n",
    "            \n",
    "        Qpi_sa[cur_obs,cur_action]   = Qpi_sa[cur_obs,cur_action] + alpha*(reward + gamma*expect_q_sum - Qpi_sa[cur_obs,cur_action])\n",
    "        cur_obs                      = next_obs\n",
    "        cur_action                   = next_action\n",
    "        \n",
    "        \n",
    "    # Policy Improvement\n",
    "    new_policy_pi  = get_policy(state_list, Qpi_sa)                \n",
    "    if new_policy_pi==policy_pi:\n",
    "        converged = True\n",
    "        \n",
    "    policy_pi = new_policy_pi \n",
    "    \n",
    "pickle.dump(policy_pi, open(\"task1_onpolicy-expectedsarsa.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d8b95e1-5fd1-4fd4-9786-3cde8996a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes took to converge for on-policy Expected SARSA:  9518\n",
      "Success rate of on-policy Expected SARSA control over 1000 runs:  7.3 %\n",
      "Average steps taken to reach the goal:  171.32876712328766\n"
     ]
    }
   ],
   "source": [
    "print('Episodes took to converge for on-policy Expected SARSA: ', episodes)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy_pi, no_actions, 1000)\n",
    "print('Success rate of on-policy Expected SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5d04a-206a-4dc8-a550-ff1b216a4d27",
   "metadata": {},
   "source": [
    "#### Task 1: TD(0) Off-Policy Expected SARSA with a greedy control policy\n",
    "Intialization of an arbitrary state-action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "70f01a0c-8a77-46f4-8b81-b63f6cd91b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qpi_sa     = {}\n",
    "state_list = [] # contains the list of states\n",
    "no_actions = 3   # 0, 1 and 2\n",
    "\n",
    "# multiplicative factor of 100\n",
    "pos_min    = -120\n",
    "pos_max    = 60\n",
    "pos_inc    = 1\n",
    "pos_fac    = 100\n",
    "\n",
    "# multiplicative factor of 100\n",
    "vel_min    = -7\n",
    "vel_max    = 7\n",
    "vel_inc    = 1\n",
    "vel_fac    = 100\n",
    "\n",
    "for pos in range(pos_min, pos_max+1, pos_inc):\n",
    "    for vel in range(vel_min, vel_max+1, vel_inc):\n",
    "        state_list.append((pos, vel))\n",
    "        for action in range(no_actions):\n",
    "            Qpi_sa[(pos, vel), action] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701f7c6e-9f87-4bb7-9999-f03ee5eefc1a",
   "metadata": {},
   "source": [
    "Initialization of behaviour (B) and target (Pi) policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d5659e7-1642-4f1e-aafe-39b519ad3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_pi = {}             # target policy\n",
    "\n",
    "for item in state_list:\n",
    "    policy_pi[item] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2\n",
    "    \n",
    "policy_b  = policy_pi      # behaviour policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb25bbd6-be9e-40fd-907e-7e354fe1b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_behaviour_policy(pol_pi, state_list, no_actions, epsilon):\n",
    "    '''\n",
    "    makes the behaviour policy exploratory with respect to policy pi\n",
    "    '''\n",
    "    pol_b = {}\n",
    "    \n",
    "    for state in state_list:\n",
    "        nonzeroind   = np.nonzero(policy_pi[state])[0]\n",
    "        pol_b[state] = [0,0,0]\n",
    "        \n",
    "        for action in range(no_actions):\n",
    "            if action not in nonzeroind:\n",
    "                pol_b[state][action] = epsilon/no_actions \n",
    "            else:\n",
    "                pol_b[state][action] = policy_pi[state][action] - ((no_actions-len(nonzeroind))/len(nonzeroind))*(epsilon/no_actions)\n",
    "                \n",
    "    return pol_b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "58cf1b0c-f1ea-46d6-ba5d-af09ee833179",
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make('MountainCar-v0')\n",
    "\n",
    "episodes    = 0\n",
    "epsilon     = 0.1\n",
    "alpha       = 0.5\n",
    "gamma       = 0.9\n",
    "converged   = False\n",
    "\n",
    "while not converged:\n",
    "    episodes += 1\n",
    "    \n",
    "    # Policy Evaluation\n",
    "    cur_obs     = env.reset()\n",
    "    cur_obs     = process_state(cur_obs, pos_fac, vel_fac)\n",
    "    cur_action  = np.random.choice(no_actions, p=policy_b[cur_obs])     # action chosen from behaviour policy\n",
    "    done        = False\n",
    "    \n",
    "    while not done:        \n",
    "        next_obs, reward, done, info = env.step(cur_action)\n",
    "        next_obs                     = process_state(next_obs, pos_fac, vel_fac)\n",
    "        next_action                  = np.random.choice(no_actions, p=policy_b[next_obs])     # action chosen from behaviour policy\n",
    "        expect_q_sum                 = 0\n",
    "        for action in range(no_actions):\n",
    "            expect_q_sum             += policy_pi[next_obs][action]* Qpi_sa[next_obs, action]\n",
    "            \n",
    "        Qpi_sa[cur_obs,cur_action]   = Qpi_sa[cur_obs,cur_action] + alpha*(reward + gamma*expect_q_sum - Qpi_sa[cur_obs,cur_action])\n",
    "        cur_obs                      = next_obs\n",
    "        cur_action                   = next_action\n",
    "        \n",
    "        \n",
    "    # Policy Improvement\n",
    "    new_policy_pi  = get_policy(state_list, Qpi_sa)                 \n",
    "    if new_policy_pi==policy_pi:\n",
    "        converged = True\n",
    "        \n",
    "    policy_pi = new_policy_pi \n",
    "    policy_b  = update_behaviour_policy(policy_pi, state_list, no_actions, epsilon)\n",
    "    \n",
    "pickle.dump(policy_pi, open(\"task1_offpolicy-expectedsarsa.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0677e5df-4d02-46d2-a2b2-e34b9ba1d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes took to converge for off-policy Expected SARSA:  9189\n",
      "Success rate of off-policy Expected SARSA control over 1000 runs:  12.5 %\n",
      "Average steps taken to reach the goal:  174.88\n"
     ]
    }
   ],
   "source": [
    "print('Episodes took to converge for off-policy Expected SARSA: ', episodes)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy_pi, no_actions, 1000)\n",
    "print('Success rate of off-policy Expected SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155183ec-4dda-427b-9359-3b4ef0dda566",
   "metadata": {},
   "source": [
    "#### Task 1 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b667b2-f2f0-4c30-85b0-24fbab454359",
   "metadata": {},
   "source": [
    "| Controller Type | Episodes Taken to Converge | Success Rate | Avg. Steps to Goal |\n",
    "| --- | --- | --- | --- |\n",
    "| On-policy SARSA | 11413 | 6.5 % | 178.57 | \n",
    "| On-policy Expected SARSA | 9518 | 7.3 % | 171.32 |\n",
    "| Off-policy Expected SARSA | 9189 | 12.5 % | 174.88 |\n",
    "\n",
    "It is evident that expected SARSA converges faster to the optimal policy as compared to SARSA. The expected versions also have better success rates. There is a significant performance difference between on-policy and off-policy expected SARSA. The off-policy version has ~71% additional success rate over on-policy. This can be attributed to sufficient exploration in off-policy expected SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40694095-93c5-417c-9996-f3f2c487a80b",
   "metadata": {},
   "source": [
    "#### Exercise 2: TD(n) - nstep SARSA control\n",
    "Contains an implementation of control algorithms that use TD(n) SARSA. The implementation is tested in Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f576a2-4ef7-4104-bd23-ec25721a8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_sarsa(epsilon, alpha, gamma, n_step): \n",
    "    \n",
    "    Qpi_sa     = {}\n",
    "    state_list = [] # contains the list of states\n",
    "    no_actions = 3   # 0, 1 and 2\n",
    "\n",
    "    # multiplicative factor of 100\n",
    "    pos_min    = -120\n",
    "    pos_max    = 60\n",
    "    pos_inc    = 1\n",
    "    pos_fac    = 100\n",
    "\n",
    "    # multiplicative factor of 100\n",
    "    vel_min    = -7\n",
    "    vel_max    = 7\n",
    "    vel_inc    = 1\n",
    "    vel_fac    = 100\n",
    "\n",
    "    for pos in range(pos_min, pos_max+1, pos_inc):\n",
    "        for vel in range(vel_min, vel_max+1, vel_inc):\n",
    "            state_list.append((pos, vel))\n",
    "            for action in range(no_actions):\n",
    "                Qpi_sa[(pos, vel), action] = 0\n",
    "                \n",
    "    policy_pi = {}\n",
    "    for item in state_list:\n",
    "        policy_pi[item] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2\n",
    "        \n",
    "        \n",
    "    env         = gym.make('MountainCar-v0')\n",
    "\n",
    "    episodes    = 0\n",
    "    converged   = False\n",
    "\n",
    "    while not converged:\n",
    "        episodes += 1\n",
    "    \n",
    "        ## Policy Evaluation ##\n",
    "        cur_obs     = env.reset()\n",
    "        cur_obs     = process_state(cur_obs, pos_fac, vel_fac)\n",
    "        cur_action  = epsilon_greedy_action_policy(cur_obs, epsilon, no_actions, policy_pi)\n",
    "        done        = False\n",
    "        t           = 0\n",
    "        t_term      = pow(10,8)              # the termination step\n",
    "        trans_list  = []\n",
    "        trans_list.append([cur_obs, cur_action])\n",
    "    \n",
    "        while not done:        \n",
    "            next_obs, reward, done, info = env.step(cur_action)     \n",
    "            if done:\n",
    "                t_term = t+1\n",
    "            next_obs                     = process_state(next_obs, pos_fac, vel_fac) \n",
    "            next_action                  = epsilon_greedy_action_policy(next_obs, epsilon, no_actions, policy_pi)\n",
    "            cur_obs                      = next_obs\n",
    "            cur_action                   = next_action\n",
    "            trans_list.append([next_obs, next_action, reward])\n",
    "        \n",
    "            tau = t-n_step+1        \n",
    "            if tau>=0:       \n",
    "                G   = 0\n",
    "                for i in range(tau+1, min((tau+n_step+1), t_term+1)):\n",
    "                    G += pow(gamma, i-tau-1)*trans_list[i][2]\n",
    "            \n",
    "                if tau+n_step<t_term:\n",
    "                    G += pow(gamma, n_step)*Qpi_sa[trans_list[tau+n_step][0], trans_list[tau+n_step][1]]\n",
    "            \n",
    "                Qpi_sa[trans_list[tau][0], trans_list[tau][1]]  = Qpi_sa[trans_list[tau][0], trans_list[tau][1]] + alpha*(G - Qpi_sa[trans_list[tau][0], trans_list[tau][1]])\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        new_policy  = get_policy(state_list, Qpi_sa)    \n",
    "        if new_policy==policy_pi:\n",
    "            converged = True\n",
    "        \n",
    "        policy_pi = new_policy\n",
    "        \n",
    "    return policy_pi, episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe668f3-12e8-4626-9519-06b7a895d395",
   "metadata": {},
   "source": [
    "#### Exercise 2: TD(n) Off-Policy Tree Backup Control\n",
    "The below implementation is tested in bonus task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58095130-a45a-4fe7-bc54-9cc015a8d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_backup(epsilon, alpha, gamma, n_step):    \n",
    "    Qpi_sa     = {}\n",
    "    state_list = []  # contains the list of states\n",
    "    no_actions = 3   # 0, 1 and 2\n",
    "\n",
    "    # multiplicative factor of 100\n",
    "    pos_min    = -120\n",
    "    pos_max    = 60\n",
    "    pos_inc    = 1\n",
    "    pos_fac    = 100\n",
    "\n",
    "    # multiplicative factor of 100\n",
    "    vel_min    = -7\n",
    "    vel_max    = 7\n",
    "    vel_inc    = 1\n",
    "    vel_fac    = 100\n",
    "\n",
    "    for pos in range(pos_min, pos_max+1, pos_inc):\n",
    "        for vel in range(vel_min, vel_max+1, vel_inc):\n",
    "            state_list.append((pos, vel))\n",
    "            for action in range(no_actions):\n",
    "                Qpi_sa[(pos, vel), action] = 0\n",
    "                \n",
    "    policy_pi = {}\n",
    "    for item in state_list:\n",
    "        policy_pi[item] = [1/no_actions, 1/no_actions, 1/no_actions] # action is 0, 1 or 2\n",
    "        \n",
    "    env         = gym.make('MountainCar-v0')\n",
    "    episodes    = 0\n",
    "    converged   = False\n",
    "\n",
    "    while not converged:\n",
    "        episodes += 1\n",
    "    \n",
    "        cur_obs     = env.reset()\n",
    "        cur_obs     = process_state(cur_obs, pos_fac, vel_fac)\n",
    "        cur_action  = epsilon_greedy_action_policy(cur_obs, epsilon, no_actions, policy_pi)\n",
    "        done        = False\n",
    "        t           = 0\n",
    "        t_term      = pow(10,8)              # the termination step\n",
    "        trans_list  = []\n",
    "        trans_list.append([cur_obs, cur_action])\n",
    "        cur_policy  = get_policy(state_list, Qpi_sa)\n",
    "    \n",
    "        while not done:        \n",
    "            next_obs, reward, done, info = env.step(cur_action)         \n",
    "            if done:\n",
    "                t_term = t+1\n",
    "            next_obs                     = process_state(next_obs, pos_fac, vel_fac)\n",
    "            next_action                  = epsilon_greedy_action_policy(next_obs, epsilon, no_actions, policy_pi)            \n",
    "            cur_obs                      = next_obs\n",
    "            cur_action                   = next_action\n",
    "            trans_list.append([next_obs, next_action, reward])\n",
    "        \n",
    "            tau = t-n_step+1        \n",
    "            if tau>=0:   \n",
    "                policy_pi   = get_policy(state_list, Qpi_sa)\n",
    "                G           = 0\n",
    "                \n",
    "                if t+1>=t_term:\n",
    "                    G += trans_list[t_term][2]\n",
    "                else:\n",
    "                    expect_q_sum  = 0\n",
    "                    for action in range(no_actions):\n",
    "                        expect_q_sum   += policy_pi[trans_list[t+1][0]][action]* Qpi_sa[trans_list[t+1][0], action]\n",
    "                    G += trans_list[t+1][2] + gamma*expect_q_sum\n",
    "                \n",
    "                for k in range(min(t, t_term-1), tau, -1):\n",
    "                    expect_sum  = 0\n",
    "                    for action in range(no_actions):\n",
    "                        if action!=trans_list[k][1]:\n",
    "                            expect_sum += policy_pi[trans_list[k][0]][action]* Qpi_sa[trans_list[k][0], action]\n",
    "                    G += trans_list[k][2] + gamma*expect_sum + gamma*policy_pi[trans_list[k][0]][trans_list[k][1]]*G\n",
    "            \n",
    "                Qpi_sa[trans_list[tau][0], trans_list[tau][1]]  = Qpi_sa[trans_list[tau][0], trans_list[tau][1]] + alpha*(G - Qpi_sa[trans_list[tau][0], trans_list[tau][1]])\n",
    "            \n",
    "            t += 1\n",
    "        \n",
    "        new_policy  = get_policy(state_list, Qpi_sa)    \n",
    "        if new_policy==policy_pi:\n",
    "            converged = True \n",
    "        policy_pi = new_policy\n",
    "    \n",
    "    return policy_pi, episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020496b-27ef-4c63-9305-9d07dc4e5da7",
   "metadata": {},
   "source": [
    "#### Task 2: TD(2), TD(3), TD(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe701346-c286-445f-b821-349d18250af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon     = 0.1\n",
    "alpha       = 0.5\n",
    "gamma       = 0.9\n",
    "no_actions  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b7b81-1d70-4ce7-936a-a4bfede642c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step      = 2\n",
    "policy, eps = n_sarsa(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task2_td2-nsarsa.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(2) SARSA: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(2) SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670e759-1440-45c7-9bf7-975b43712478",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step      = 3\n",
    "policy, eps = n_sarsa(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task2_td3-nsarsa.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(3) SARSA: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(3) SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed13b7-19c4-4a79-9179-44edf3d75d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step      = 4\n",
    "policy, eps = n_sarsa(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task2_td4-nsarsa.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(4) SARSA: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(4) SARSA control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047bf4f-1a12-4a0e-bfcf-7d6a062778d7",
   "metadata": {},
   "source": [
    "#### Bonus Task 3: Tree Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44c3f41-3ce6-481d-b849-4aa89f0ef24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon     = 0.1\n",
    "alpha       = 0.5\n",
    "gamma       = 0.9\n",
    "no_actions  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f39b37f-67a4-44b7-a421-808d5397dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes took to converge for TD(2) tree backup:  1\n",
      "Success rate of TD(2) tree backup control over 1000 runs:  0.0 %\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;34m/opt/anaconda3/envs/milamoth/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m, in \u001b[0;32mrun_code\u001b[0m:\nLine \u001b[0;34m3441\u001b[0m:  exec(code_obj, \u001b[36mself\u001b[39;49;00m.user_global_ns, \u001b[36mself\u001b[39;49;00m.user_ns)\n",
      "In  \u001b[0;34m[12]\u001b[0m:\nLine \u001b[0;34m7\u001b[0m:     \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAverage steps taken to reach the goal: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36msum\u001b[39;49;00m(steps)/\u001b[36mlen\u001b[39;49;00m(steps))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "n_step      = 2\n",
    "policy, eps = tree_backup(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task3_td2-treebackup.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(2) tree backup: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(2) tree backup control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fa08f-df1a-40b4-bfaa-fff0bdc4948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step      = 3\n",
    "policy, eps = tree_backup(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task3_td3-treebackup.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(3) tree backup: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(3) tree backup control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce22ad4-9a8e-43ae-b831-635f5e805954",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step      = 4\n",
    "policy, eps = tree_backup(epsilon, alpha, gamma, n_step)\n",
    "pickle.dump(policy, open(\"task3_td4-treebackup.p\", \"wb\" ))\n",
    "print('Episodes took to converge for TD(4) tree backup: ', eps)\n",
    "success, steps = control_performance('MountainCar-v0', pos_fac, vel_fac, policy, no_actions, 1000)\n",
    "print('Success rate of TD(4) tree backup control over 1000 runs: ', (success/1000)*100, '%')\n",
    "print('Average steps taken to reach the goal: ', sum(steps)/len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809e6c3-6e93-445a-9408-ca0e4d703156",
   "metadata": {},
   "source": [
    "**COMMENT**: I have implemented n-step SARSA and n-step Tree Backup, however, there are some convergence issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
