{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HsGqubiDkQnd"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7iS40R9okStg"
   },
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.74\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing a arbitrary value function\n",
    "import copy\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "no_states  = 16\n",
    "no_actions = 4\n",
    "Vpi_s      = {}\n",
    "\n",
    "for i in range(no_states):\n",
    "    Vpi_s[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing a random policy function\n",
    "Policy = {}\n",
    "\n",
    "for i in range(16):\n",
    "    Policy[i] = [0.25, 0.25, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_evaluation(discount, threshold, env_model):\n",
    "    '''\n",
    "    performs one sweep of truncated policy evaluation for value iteration\n",
    "    '''\n",
    "    global Vpi_s, Policy\n",
    "    \n",
    "    discount   = discount   # discount factor\n",
    "    threshold  = threshold  # threshold for terminating policy evaluation\n",
    "    iterations = 0          # no. of iterations it took to converge\n",
    "    converged  = False      # flag to exit while loop when converged\n",
    "    env_model  = env_model  # model of the environment i.e. transition probabilities\n",
    "    no_actions = 4\n",
    "\n",
    "    while not converged:\n",
    "        iterations += 1\n",
    "        max_diff = 0\n",
    "        for state in Vpi_s.keys():\n",
    "            cur_value    = copy.copy(Vpi_s[state])\n",
    "        \n",
    "            qpi_list = []                     # contains q(s,a) for every action\n",
    "            for action in range(no_actions):\n",
    "                reward       = env_model[state][action][0][2]\n",
    "                next_state   = env_model[state][action][0][1]\n",
    "                trans_prob   = env_model[state][action][0][0]            \n",
    "                qpi_list.append(trans_prob*(reward + discount*Vpi_s[next_state]))\n",
    "                \n",
    "            Vpi_s[state] = max(qpi_list)            \n",
    "            max_diff = max(max_diff, abs(cur_value - Vpi_s[state]))\n",
    "        \n",
    "        if max_diff<threshold: \n",
    "            converged = True \n",
    "            \n",
    "    return iterations\n",
    "            \n",
    "def policy_improvement(discount, env_model):\n",
    "    '''\n",
    "    performs one sweep of policy improvement\n",
    "    '''\n",
    "    global Vpi_s, Policy\n",
    "    \n",
    "    converged  = True\n",
    "    no_actions = 4\n",
    "    \n",
    "    for state in Vpi_s.keys():\n",
    "        cur_stateaction = copy.copy(Policy[state])   # current qpi(s,a)\n",
    "        \n",
    "        qpi_list = []                     # contains q(s,a) for every action\n",
    "        for action in range(no_actions):\n",
    "            reward       = env_model[state][action][0][2]\n",
    "            next_state   = env_model[state][action][0][1]\n",
    "            trans_prob   = env_model[state][action][0][0]\n",
    "            qpi_list.append(trans_prob*(reward + discount*Vpi_s[next_state]))\n",
    "            \n",
    "        maxa_list      = np.argwhere(qpi_list == np.amax(qpi_list))\n",
    "        maxa_list_indx = []\n",
    "        \n",
    "        # indices that have max. q values\n",
    "        for item in maxa_list:\n",
    "            maxa_list_indx.append(item[0])\n",
    "            \n",
    "        # updating the policy\n",
    "        for i in range(no_actions):\n",
    "            if i in maxa_list_indx:\n",
    "                Policy[state][i] = 1/len(maxa_list_indx)\n",
    "            else:\n",
    "                Policy[state][i] = 0 \n",
    "                \n",
    "        if Policy[state]!=cur_stateaction:\n",
    "            converged = False\n",
    "            \n",
    "    return converged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Loop\n",
    "p_iterations = 0        # no of policy iteration steps\n",
    "eval_iter    = 0        # total no of evaluation iterations\n",
    "discount     = 0.9\n",
    "threshold    = 0.0001\n",
    "\n",
    "while True:\n",
    "    eval_steps   = truncated_policy_evaluation(discount, threshold, env.env.P)\n",
    "    converged    = policy_improvement(discount, env.env.P)\n",
    "    p_iterations += 1    \n",
    "    eval_iter    += eval_steps\n",
    "    \n",
    "    if converged:\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy:  {\n",
      "   \"0\": [\n",
      "      0,\n",
      "      0.5,\n",
      "      0.5,\n",
      "      0\n",
      "   ],\n",
      "   \"1\": [\n",
      "      0,\n",
      "      0,\n",
      "      1.0,\n",
      "      0\n",
      "   ],\n",
      "   \"2\": [\n",
      "      0,\n",
      "      1.0,\n",
      "      0,\n",
      "      0\n",
      "   ],\n",
      "   \"3\": [\n",
      "      1.0,\n",
      "      0,\n",
      "      0,\n",
      "      0\n",
      "   ],\n",
      "   \"4\": [\n",
      "      0,\n",
      "      1.0,\n",
      "      0,\n",
      "      0\n",
      "   ],\n",
      "   \"5\": [\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25\n",
      "   ],\n",
      "   \"6\": [\n",
      "      0,\n",
      "      1.0,\n",
      "      0,\n",
      "      0\n",
      "   ],\n",
      "   \"7\": [\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25\n",
      "   ],\n",
      "   \"8\": [\n",
      "      0,\n",
      "      0,\n",
      "      1.0,\n",
      "      0\n",
      "   ],\n",
      "   \"9\": [\n",
      "      0,\n",
      "      0.5,\n",
      "      0.5,\n",
      "      0\n",
      "   ],\n",
      "   \"10\": [\n",
      "      0,\n",
      "      1.0,\n",
      "      0,\n",
      "      0\n",
      "   ],\n",
      "   \"11\": [\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25\n",
      "   ],\n",
      "   \"12\": [\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25\n",
      "   ],\n",
      "   \"13\": [\n",
      "      0,\n",
      "      0,\n",
      "      1.0,\n",
      "      0\n",
      "   ],\n",
      "   \"14\": [\n",
      "      0,\n",
      "      0,\n",
      "      1.0,\n",
      "      0\n",
      "   ],\n",
      "   \"15\": [\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25,\n",
      "      0.25\n",
      "   ]\n",
      "}\n",
      "Final State-Value Function:  {\n",
      "   \"0\": 0.5904900000000002,\n",
      "   \"1\": 0.6561000000000001,\n",
      "   \"2\": 0.7290000000000001,\n",
      "   \"3\": 0.6561000000000001,\n",
      "   \"4\": 0.6561000000000001,\n",
      "   \"5\": 0.0,\n",
      "   \"6\": 0.81,\n",
      "   \"7\": 0.0,\n",
      "   \"8\": 0.7290000000000001,\n",
      "   \"9\": 0.81,\n",
      "   \"10\": 0.9,\n",
      "   \"11\": 0.0,\n",
      "   \"12\": 0.0,\n",
      "   \"13\": 0.9,\n",
      "   \"14\": 1.0,\n",
      "   \"15\": 0.0\n",
      "}\n",
      "Total number of value iteration steps:  2\n",
      "Total number of policy evaluation steps:  8\n"
     ]
    }
   ],
   "source": [
    "print('Final Policy: ', json.dumps(Policy, indent=3))\n",
    "print('Final State-Value Function: ', json.dumps(Vpi_s, indent=3))\n",
    "print('Total number of value iteration steps: ', p_iterations)\n",
    "print('Total number of policy evaluation steps: ', eval_iter)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A1_FrozenLake.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
